{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheLastCD/Dissertation-Final/blob/main/diss_program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMzhEP7B14og"
      },
      "source": [
        "# Pip Install\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kZqiQEuEtHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467190a4-7e02-430a-f3c2-456aaf5bba06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_text==2.8.1 in /usr/local/lib/python3.7/dist-packages (2.8.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text==2.8.1) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text==2.8.1) (2.8.2+zzzcolab20220523105045)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.46.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.26.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.0.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.2.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.15.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (4.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_text==2.8.1\n",
        "#!pip install tensorflow_ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPcCyybFsVk2"
      },
      "source": [
        "# imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q10FScHFCT61"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "#import tensorflow_ranking as tfr\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "#import tensorflow_ranking as tfr\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import random\n",
        "from sklearn.cluster import KMeans\n",
        "from itertools import chain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "493VrdwnJYFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb0e1a4-7ad0-47f0-f6d6-56838d97c00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.2\n",
            "Running on TPU  ['10.76.36.58:8470']\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.76.36.58:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.76.36.58:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.76.36.58:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.76.36.58:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function initialize_tpu_system.<locals>._tpu_init_fn at 0x7f653edfe560> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function initialize_tpu_system.<locals>._tpu_init_fn at 0x7f653edfe560> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function initialize_tpu_system.<locals>._tpu_init_fn at 0x7f653edfe560> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3nKkMdwMAQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e589bbc1-7e3e-4135-b806-41eab3029a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ],
      "source": [
        "with tpu_strategy.scope(): \n",
        "  load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "  module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "  model = hub.load(module_url, options = load_locally)\n",
        "  print (\"module %s loaded\" % module_url)\n",
        "  def embed(input):\n",
        "    return model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7bj0PCvF6S9"
      },
      "source": [
        "# Translation Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqdvX54oCh8K"
      },
      "outputs": [],
      "source": [
        "#Text Preprocessing\n",
        "#Standardization: standardizes input text\n",
        "# first by removing accented words and replacing them with its ascii equivalent\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V3R1wpvI-h-"
      },
      "outputs": [],
      "source": [
        "# The Encoder\n",
        "# the encoder takes the list of tokenIDs\n",
        "# then looks  up an embedding vector for each token\n",
        "# processes the embeddings into a new sequence\n",
        "# returning: \n",
        "    # a processed sequence (To be passed to the Attention head)\n",
        "    # Internal state (Used to initialise the decoder)\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B36wV2VTJGgA"
      },
      "outputs": [],
      "source": [
        "# The attention head\n",
        "# The decoder uses attention to selectively focus on parts of the input sequence\n",
        "# The attention takes a series of vectors as input for each example and returns an attention vector for each example\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "\n",
        "     # From Eqn. (4), `W1@ht`.\n",
        "     w1_query = self.W1(query)\n",
        "\n",
        "     # From Eqn. (4), `W2@hs`.\n",
        "     w2_key = self.W2(value)\n",
        "\n",
        "     query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "     value_mask = mask\n",
        "\n",
        "     context_vector, attention_weights = self.attention(\n",
        "         inputs = [w1_query, value, w2_key],\n",
        "         mask=[query_mask, value_mask],\n",
        "         return_attention_scores = True,\n",
        "     )\n",
        "\n",
        "\n",
        "     return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDtQvDDnJ9A1"
      },
      "outputs": [],
      "source": [
        "# The Decoder\n",
        "# The decoder will generate predictions for the next output token\n",
        "# it is given the complete output of the encoder\n",
        "# using an RNN it keeps track of what it has generated so far\n",
        "# the RNN output is used as a query to the attention over the encoders output, producing thecontext vector\n",
        "# A combination of the context vector and the RNN output is used to generate the attention vector\n",
        "# it generates logit predictions for the next based on the attention vector\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "    \n",
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any\n",
        "\n",
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "    \n",
        "          # Step 1. Lookup the embeddings\n",
        "          vectors = self.embedding(inputs.new_tokens)\n",
        "        \n",
        "          # Step 2. Process one step with the RNN\n",
        "          rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "    \n",
        "        \n",
        "          # Step 3. Use the RNN output as the query for the attention over the\n",
        "          # encoder output.\n",
        "          context_vector, attention_weights = self.attention(\n",
        "              query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "        \n",
        "          # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "          #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "          context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "        \n",
        "          # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "          attention_vector = self.Wc(context_and_rnn_output)\n",
        "        \n",
        "          # Step 5. Generate logit predictions:\n",
        "          logits = self.fc(attention_vector)\n",
        "          return DecoderOutput(logits, attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1u8l1XrKD9m"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9vWd_krKKaf"
      },
      "outputs": [],
      "source": [
        "# Implementing the Training Step\n",
        "\n",
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "def _preprocess(self, input_text, target_text):\n",
        "\n",
        "    \n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "    \n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "    \n",
        "    target_mask = target_tokens != 0\n",
        "    \n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "TrainTranslator._preprocess = _preprocess\n",
        "\n",
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}\n",
        "\n",
        "\n",
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state\n",
        "\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)\n",
        "\n",
        "\n",
        "# Train Model\n",
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxRjqTyHKSlP"
      },
      "outputs": [],
      "source": [
        "# Translate\n",
        "# similar to  the training Loop except the input to decoder at each time step is a sample from the decoders last prediction\n",
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    # find these tags and produce a mask\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "    \n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dPVjgDlKbp7"
      },
      "outputs": [],
      "source": [
        "# Convert tokens IDs to text\n",
        "def tokens_to_text(self, result_tokens):\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  return result_text\n",
        "\n",
        "\n",
        "# Sample from the decoders predictions\n",
        "def sample(self, logits, temperature):\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  # logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "  #print(logits)\n",
        "  logits = tf.where(self.token_mask, -5, logits)\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    # so this is where it selects the best output\n",
        "    # so maybe find my output in this then find the distance between what it picked and the potential outputs\n",
        "    # need to find a way to find my input in the logits thing\n",
        "    new_tokens = tf.random.categorical(logits/temperature,num_samples=1)\n",
        "  return new_tokens,logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CdFz1GxKfrC"
      },
      "outputs": [],
      "source": [
        "# look for the an array of just predictions\n",
        "# Implement the translation loop\n",
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "    \n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "    attention.append(dec_result.attention_weights)\n",
        "    new_tokens,logits = self.sample(dec_result.logits, temperature)\n",
        "    \n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJnqyQDiKjAb"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10),dpi = 500)\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJRP6C1dGDdo"
      },
      "source": [
        "# Data Importing and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0YjJ-XX7QZc"
      },
      "outputs": [],
      "source": [
        "#Loading Data\n",
        "# this function: \n",
        "    # adds a start and end token to each sentence, \n",
        "    # removes all the special characters\n",
        "    # Construct a word index and reverse word index (allows a word to be found by an id and an ide to be found from a word)\n",
        "    # Pad each sentence to a maximum \n",
        "def load_data(path):\n",
        "  # text = path.read_text(encoding='utf-8')\n",
        "  text = path.read_text(encoding = 'latin-1')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  # pairs = [line.split('\\t') for line in lines]\n",
        "  pairs = [line.split('\\n') for line in lines]\n",
        "\n",
        "\n",
        "\n",
        "  # inp = [inp for targ, inp in pairs]\n",
        "  # targ = [targ for targ, inp in pairs]\n",
        "  targ = []\n",
        "  inp = []\n",
        "  for i in pairs:\n",
        "    try:\n",
        "      if (i != ['']):\n",
        "          findspeak = i[0].split(\",\")\n",
        "          if findspeak[0] == \"INTERVIEWER\":\n",
        "              inp.append(findspeak[1].replace('\"', \"\"))\n",
        "          else:\n",
        "              targ.append(findspeak[1].replace('\"', \"\"))\n",
        "    except:\n",
        "      x = i\n",
        "\n",
        "\n",
        "  print(\"Utterance Pairs: \", len(inp))\n",
        "  return targ, inp\n",
        "\n",
        "def checkEmpty(tar,inp):\n",
        "  ranger = len(inp)\n",
        "  for i in range(0,ranger):\n",
        "    try:\n",
        "      if len(targ[i].split()) ==0:\n",
        "        inp.pop(i)\n",
        "        targ.pop(i)\n",
        "    except IndexError:\n",
        "      print(\"empty's removed\")\n",
        "      break\n",
        "  return targ,inp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HykFPfPYDGYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a985ffb6-4e9f-4a18-bdbc-f7639e65aa43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utterance Pairs:  3320\n",
            "empty's removed\n",
            "New Utterance Pairs:  3319\n"
          ]
        }
      ],
      "source": [
        "#Data Importing\n",
        "use_builtins = True\n",
        "\n",
        "#Download the dataset\n",
        "import pathlib\n",
        "\n",
        "path_to_file = pathlib.Path()/\"drive\"/\"MyDrive\"/\"Master.txt\"\n",
        "targ, inp = load_data(path_to_file)\n",
        "targ,inp = checkEmpty(targ.copy(),inp.copy())\n",
        "print(\"New Utterance Pairs: \", len(inp))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLiEpBo3Dive",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "db382cbf-aa99-4656-d6ca-828a1e8e2dbf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-9b617d651469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Text Vectorisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, batch_size, drop_remainder, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   1712\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1713\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1714\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mBatchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m       return ParallelBatchDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, batch_size, drop_remainder, name)\u001b[0m\n\u001b[1;32m   4877\u001b[0m         drop_remainder, dtype=dtypes.bool, name=\"drop_remainder\")\n\u001b[1;32m   4878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4879\u001b[0;31m     \u001b[0mconstant_drop_remainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_remainder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4880\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4881\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconstant_drop_remainder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    867\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnimplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       \u001b[0;31m# Some EagerTensors may not implement .numpy/resolve, e.g. parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __inference_restored_function_body_13500}} assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.]\n\t [[{{node Assert/Assert}}]]"
          ]
        }
      ],
      "source": [
        "#Create TF Dataset\n",
        "# shuffles the data and batches them efficiently\n",
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 512\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Text Vectorisation\n",
        "# Standardisation is wrapped in a text vectorization layer which handles vocab extraction and conversion of inputs to sequence tokens\n",
        "max_vocab_size = 10000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "\n",
        "# Encoder and Decoder Model\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Fma4keu3PU"
      },
      "source": [
        "# Test Train Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZMpWfb4tNDv"
      },
      "outputs": [],
      "source": [
        "def testTrainSplit(targ,inp, testSize):\n",
        "  testlength = int(len(inp)*testSize)\n",
        "  indexes = []\n",
        "  for i in range(0,testlength):\n",
        "    index = random.randint(0, len(inp)-1)\n",
        "    if index in indexes:\n",
        "      index = random.randint(0, len(inp)-1)\n",
        "    else:\n",
        "      indexes.append(index)\n",
        "  testtarg = []\n",
        "  testinp = []\n",
        "  traintarg = targ.copy()\n",
        "  traininp = inp.copy()\n",
        "  for i in indexes:\n",
        "    testtarg.append(targ[i])\n",
        "    testinp.append(inp[i])\n",
        "    traintarg.remove(targ[i])\n",
        "    traininp.remove(inp[i])\n",
        "  \n",
        "  return traintarg, traininp, testtarg, testinp\n",
        "\n",
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \" \" \n",
        "    \n",
        "    # return string  \n",
        "    return (str1.join(s))\n",
        "\n",
        "def GenMRRQuestion(targ,wordList):\n",
        "  masterResponses = []\n",
        "  masterAnswers = []\n",
        "  for i in targ:\n",
        "    answerLocation = -1\n",
        "    responses = []\n",
        "    responses.append(i)\n",
        "\n",
        "    for j in range(0,random.randint(1,10)):\n",
        "      randomWords = []\n",
        "      for x in range(0,len(i.split())):\n",
        "        word = random.randint(3,len(wordList)-1)\n",
        "        \n",
        "        randomWords.append(wordList[word])\n",
        "      y = listToString(randomWords)\n",
        "      if len(y.split()) != len(i.split()):\n",
        "        print(\"error Mismatch when joining\")\n",
        "        input()\n",
        "      if len(y.split()) == 0:\n",
        "        print(\"empty response\")\n",
        "\n",
        "      responses.append(y)\n",
        "\n",
        "    \n",
        "    #shuffle and append to masters\n",
        "    random.shuffle(responses)\n",
        "    answerLocation = responses.index(i)\n",
        "    masterResponses.append(responses)\n",
        "    masterAnswers.append(answerLocation)\n",
        "  return masterResponses,masterAnswers\n",
        "    \n",
        "\n",
        "def selectTraining(targ,length):\n",
        "  indexes = []\n",
        "  for i in range(0,length):\n",
        "    index = random.randint(0, len(targ)-1)\n",
        "    if index in indexes:\n",
        "      index = random.randint(0, len(targ)-1)\n",
        "    else:\n",
        "      indexes.append(index)\n",
        "  training = []\n",
        "  for i in indexes:\n",
        "    training.append(targ[i])\n",
        "  \n",
        "  return indexes ,training\n",
        "\n",
        "def getQuestions():\n",
        "  x =0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El5Zz7LjwT0l"
      },
      "outputs": [],
      "source": [
        "#Splitter\n",
        "print(\"old training size \", len(inp))\n",
        "targ, inp, testtarg, testinp = testTrainSplit(targ,inp,0.2)\n",
        "#implement cross validation maybe?\n",
        "\n",
        "print(\"new training size \", len(inp))\n",
        "\n",
        "#Responses is the words\n",
        "#Answers is the index\n",
        "#testing as training\n",
        "indexes, trainingResponses = selectTraining(targ,len(testtarg))\n",
        "trainingResponses, trainingAnswers = GenMRRQuestion(trainingResponses,output_text_processor.get_vocabulary())\n",
        "trainingQuestions = []\n",
        "for i in indexes:\n",
        "  trainingQuestions.append([inp[i]])\n",
        "\n",
        "#testing as testing middle ground\n",
        "testingResponses, testingAnswers = GenMRRQuestion(testtarg,output_text_processor.get_vocabulary())\n",
        "\n",
        "testingQuestions = []\n",
        "for i in testinp:\n",
        "  testingQuestions.append([i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me9mp6D2GNxP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6MU8K-gDvS-"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "TrainTranslator._train_step = _train_step\n",
        "TrainTranslator._loop_step = _loop_step\n",
        "\n",
        "\n",
        "\n",
        "#  Test the Training Step\n",
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "with tpu_strategy.scope(): \n",
        "  translator.compile(\n",
        "      optimizer=tf.optimizers.Adam(),\n",
        "      loss=MaskedLoss()\n",
        "  )\n",
        "np.log(output_text_processor.vocabulary_size())\n",
        "\n",
        "TrainTranslator._tf_train_step = _tf_train_step\n",
        "translator.use_tf_function = True\n",
        "\n",
        "# with tf.device('/device:GPU:0'):\n",
        "\n",
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "\n",
        "\n",
        "#train_translator.fit(dataset, epochs=10, workers = 10, callbacks=[batch_loss])\n",
        "train_translator.fit(dataset, epochs=10, workers = 10, callbacks=[batch_loss], use_multiprocessing = True)\n",
        "\n",
        "# checkpoint = tf.train.Checkpoint(model=train_translator)\n",
        "# local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "# checkpoint.write(checkpoint_path, options=local_device_option)\n",
        "# plt.plot(batch_loss.logs)\n",
        "# plt.ylim([0, 3])\n",
        "# plt.xlabel('Batch #')\n",
        "# plt.ylabel('CE/token')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "251qmhvFbIGn"
      },
      "outputs": [],
      "source": [
        "checkpoint = tf.train.Checkpoint(model=train_translator)\n",
        "local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "checkpoint.write(checkpoint_path, options=local_device_option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4Anv0oGWvk"
      },
      "source": [
        "# Input Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn4rniN7Dzro"
      },
      "outputs": [],
      "source": [
        "#Input Parsing\n",
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")\n",
        "\n",
        "Translator.tokens_to_text = tokens_to_text\n",
        "Translator.sample = sample\n",
        "Translator.translate = translate_unrolled\n",
        "# Translator.translate = translate_symbolic\n",
        "\n",
        "\n",
        "\n",
        "input_text = tf.constant([\n",
        "    # It is yes,\n",
        "    'Its a very unusual film youll agree with that',\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "result = translator.translate(input_text)\n",
        "print(\"Rawr Output: \")\n",
        "print(result)\n",
        "print(\"Decoded output: \")\n",
        "print(result[\"text\"].numpy()[0].decode())\n",
        "print()\n",
        "\n",
        "# Visualise the Process\n",
        "\n",
        "a = result['attention'][0]\n",
        "\n",
        "# print(np.sum(a, axis=-1))\n",
        "\n",
        "_ = plt.bar(range(len(a[0, :])), a[0, :])\n",
        "plt.imshow(np.array(a), vmin=0.0)\n",
        "\n",
        "plot_attention(result['attention'][0], input_text[0], result['text'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcfutLUNGcLo"
      },
      "source": [
        "# MRR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO5xgFOHKtFp"
      },
      "source": [
        "MRR Preamble and support functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR7O_w9SKq5g"
      },
      "outputs": [],
      "source": [
        "def find_logit(self, input_text, responses, *, max_length=50, return_attention=True, temperature,multi):\n",
        "  #temperature= 1.0\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "  \n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  result_logits = []\n",
        "  token_logits = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for i in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "    \n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "    attention.append(dec_result.attention_weights)\n",
        "    if(multi):\n",
        "      new_tokens,logits_new = self.sample(self = self, logits = dec_result.logits, temperature = temperature)\n",
        "    else:\n",
        "      new_tokens,logits_new = self.sample(dec_result.logits, temperature)\n",
        "    \n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "    \n",
        "    # Collect the generated tokens\n",
        "    word = []\n",
        "    result_tokens.append(new_tokens)\n",
        "    result_logits.append(logits_new.numpy()[tuple([0,new_tokens])])\n",
        "    if i < len(responses):   \n",
        "        for j in responses[i]:\n",
        "            word.append(logits_new.numpy()[tuple([0,j])])\n",
        "        token_logits.append(word)\n",
        "    else:\n",
        "        for k in range(len(np.array(token_logits).transpose())):\n",
        "            word.append(logits_new.numpy()[tuple([0,1])])\n",
        "        token_logits.append(word)\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  # needs a way of accounting for if the option is longer than the generated response\n",
        "  # currently only accounts for if the response is longer than the option\n",
        "  return tf.concat(np.array(token_logits).transpose(),axis = 0),tf.concat(result_logits,axis =-1)\n",
        "\n",
        "def text_to_token(wordList,text):\n",
        "    # this code creates a list of tokens for the possible responses\n",
        "    listOfToken = []\n",
        "    for i in text:\n",
        "        currentResponse = []\n",
        "        for j in i.split():\n",
        "            # for padding\n",
        "            # shows as -5 when in the logits\n",
        "            if j == \"[UNK]\":\n",
        "                currentResponse.append(wordList.index(j))\n",
        "            else:\n",
        "                try:\n",
        "                    currentResponse.append(wordList.index(j))\n",
        "                except:\n",
        "                    try:\n",
        "                        \n",
        "                        currentResponse.append(wordList.index(j.lower()))\n",
        "                    except:\n",
        "                        currentResponse.append(wordList.index(\"[UNK]\"))\n",
        "        listOfToken.append(currentResponse)\n",
        "    # transpose the array so that the first letter of each response is at the top\n",
        "    x= np.array(listOfToken)\n",
        "    # print(x.shape)\n",
        "    return x.transpose()\n",
        "\n",
        "\n",
        "def softmax(values):\n",
        "    summation = 0\n",
        "    for value in values:\n",
        "        summation += math.exp(value)\n",
        "    \n",
        "    prob_distribution = []\n",
        "    for value in values:\n",
        "        probability = math.exp(value)/summation\n",
        "        prob_distribution.append(probability)\n",
        "    \n",
        "    return prob_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlRcXsYwGpnX"
      },
      "source": [
        "MRR and Softmax Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az_u7KeAluiO"
      },
      "outputs": [],
      "source": [
        "def calcRR(model,question,responses,ansindex,multi):\n",
        "\n",
        "    question = tf.constant(question)\n",
        "    temperature = 1.0 \n",
        "    # redefine translate so that it returns probabilities\n",
        "    model.translate = find_logit\n",
        "    # get the wordlist for the model\n",
        "    wordList = model.output_text_processor.get_vocabulary()\n",
        "    # tokenises the possible responses\n",
        "    tokens = tf.constant(text_to_token(wordList,responses))\n",
        "    # find the probabilities of specific words in specific order\n",
        "    token_logits, result_logits = model.translate(model,input_text = question,responses = tokens,temperature = temperature, multi = multi )\n",
        "    # normalise the output data to between 0 and 1 \n",
        "    # calculate the mean of possible responses\n",
        "    token_means = []\n",
        "\n",
        "    # do np.mean on set axis\n",
        "    for i in token_logits:\n",
        "        x = np.mean(np.array(i))\n",
        "        token_means.append(x)\n",
        "    soft_means = softmax(token_means)\n",
        "    indexed_means = []\n",
        "    for i in range(0,len(soft_means)):\n",
        "      indexed_means.append([soft_means[i],i])\n",
        "\n",
        "    # sort means and calulate index ranking\n",
        "    sorted_list = sorted(indexed_means, key=lambda x:x[0])\n",
        "    ranking = np.array(sorted_list).transpose()[1].tolist()\n",
        "    \n",
        "    # calculate Reciprocal Rank \n",
        "    reciprocal_rank = 0\n",
        "    step = 1/len(ranking)\n",
        "    reciprocal_rank =  1-(step * ranking.index(ansindex))\n",
        "\n",
        "    return float('{:g}'.format(float('{:.4g}'.format(reciprocal_rank))))\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOVIwTqXGnag"
      },
      "source": [
        "MRR function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4RvxtLHCmR_"
      },
      "outputs": [],
      "source": [
        "def MRR(translator,responses,questions,answers,multi):\n",
        "  \n",
        "  Reciprocals =[]\n",
        "  for i  in range(1,len(responses)):\n",
        "    Reciprocals.append(calcRR(translator,questions[i],responses[i],answers[i],multi))\n",
        "  MRR = np.mean(np.array(Reciprocals))\n",
        "  # print(\"MRR = \", MRR)\n",
        "  return MRR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUx-5mMBGe1w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "373d704c-ba70-4118-89aa-38c7c3445dc0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-11e807524d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtpu_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   translator = Translator(\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_translator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_translator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0minput_text_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_translator' is not defined"
          ]
        }
      ],
      "source": [
        "# try:\n",
        "# with tpu_strategy.scope():\n",
        "#   translator = Translator(\n",
        "#       encoder=train_translator.encoder,\n",
        "#       decoder=train_translator.decoder,\n",
        "#       input_text_processor=input_text_processor,\n",
        "#       output_text_processor=output_text_processor,\n",
        "#   )\n",
        "\n",
        "# Translator.tokens_to_text = tokens_to_text\n",
        "# Translator.sample = sample\n",
        "# Translator.translate = translate_unrolled\n",
        "\n",
        "# #MRR(translator,trainingResponses,trainingQuestions,trainingAnswers,False)\n",
        "# MRR(translator,testingResponses,testingQuestions,testingAnswers,False)\n",
        "# except:\n",
        "#   print(\"training on all data has not been done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8ET5KR3eAQK"
      },
      "source": [
        "#Clustering implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV_RCjphd-3f"
      },
      "outputs": [],
      "source": [
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \" \" \n",
        "    \n",
        "    # return string  \n",
        "    return (str1.join(s))\n",
        "def GroupAndUSE(targ,inp,diaSize):\n",
        "  groups = []\n",
        "  dialoguesI = []\n",
        "  dialoguesT = []\n",
        "  tempHold = []\n",
        "  tempHoldT = []\n",
        "  tempHoldI = []\n",
        "  for i in range(0,len(inp)):\n",
        "      #record all utterance pairs\n",
        "      tempHoldI.append(inp[i])\n",
        "      tempHoldT.append(targ[i])\n",
        "      tempHold.append(inp[i])\n",
        "      tempHold.append(targ[i])\n",
        "      #when enough utterance pairs have been appended it is then appended to groups\n",
        "      if i % diaSize == diaSize-1:\n",
        "          groups.append(listToString(tempHold))\n",
        "          dialoguesI.append(tempHoldI)\n",
        "          dialoguesT.append(tempHoldT)\n",
        "          tempHold =[]\n",
        "          tempHoldT = []\n",
        "          tempHoldI = []\n",
        "  #take the remaining utterance pairs and add them on the end if theirs any remaining\n",
        "  if len(tempHold) != 0:\n",
        "    groups.append(listToString(tempHold))\n",
        "    dialoguesI.append(tempHoldI)\n",
        "    dialoguesT.append(tempHoldT)\n",
        "\n",
        "\n",
        "  group_embeddings = embed(groups)\n",
        "  similarity = np.inner(group_embeddings, group_embeddings)\n",
        "  return similarity, dialoguesI, dialoguesT\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6MA-BwhrhDa",
        "outputId": "2f706057-4838-444d-d9c9-9e345a06bee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inertia 47912.7109375\n"
          ]
        }
      ],
      "source": [
        "#heatmap of all dialogues and dialogues as 2D array\n",
        "similarity, newInp, newTarg = GroupAndUSE(targ,inp,1)\n",
        "clusters = 2\n",
        "#run K means on that heatmap\n",
        "kmeans = KMeans(n_clusters=clusters, random_state=0).fit(similarity)\n",
        "print(\"Inertia\", kmeans.inertia_)\n",
        "LCDT = []\n",
        "LCDI = []\n",
        "for i in range(0,clusters):\n",
        "  #extract clusters form labels\n",
        "  result = np.where(kmeans.labels_ == i)\n",
        "  indices = []\n",
        "  clusteredTarg = [newTarg[i] for i in result[0]]\n",
        "  clusteredInp = [newInp[i] for i in result[0]]\n",
        "  LCDT.append(clusteredTarg)\n",
        "  LCDI.append(clusteredInp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVAK4LJuZCQD"
      },
      "source": [
        "# Training On Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZkWDsJlmZYM8",
        "outputId": "d99eb9ec-ff95-4dc3-e199-aa4ade75ac82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model number:  1\n",
            "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f4c574e6ef0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f4c574e6ef0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f4c574e6ef0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f4c573e1830> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f4c573e1830> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f4c573e1830> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f4c573884d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f4c573884d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f4c573884d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c103be57f521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchLogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0mtrain_translator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   translator = Translator(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d07fbc0c70e9>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tf_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d07fbc0c70e9>\u001b[0m in \u001b[0;36m_tf_train_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_tf_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d07fbc0c70e9>\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0;31m# Pass in two tokens from the target sequence:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# 1. The current input to the decoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature."
          ]
        }
      ],
      "source": [
        "clusterModels =[]\n",
        "for i in range(0,len(LCDI)):\n",
        "  print(\"model number: \", i+1)\n",
        "  inp = list(chain.from_iterable(LCDI[i]))\n",
        "  targ = list(chain.from_iterable(LCDT[i]))\n",
        "  BUFFER_SIZE = len(inp)\n",
        "  BATCH_SIZE = 16\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  max_vocab_size = 5000\n",
        "\n",
        "  input_text_processor = tf.keras.layers.TextVectorization(\n",
        "      standardize=tf_lower_and_split_punct,\n",
        "      max_tokens=max_vocab_size)\n",
        "\n",
        "  input_text_processor.adapt(inp)\n",
        "\n",
        "\n",
        "  output_text_processor = tf.keras.layers.TextVectorization(\n",
        "      standardize=tf_lower_and_split_punct,\n",
        "      max_tokens=max_vocab_size)\n",
        "\n",
        "  output_text_processor.adapt(targ)\n",
        "\n",
        "  embedding_dim = 256\n",
        "  units = 1024\n",
        "\n",
        "  Decoder.call = call\n",
        "  #Training\n",
        "  TrainTranslator._train_step = _train_step\n",
        "  TrainTranslator._loop_step = _loop_step\n",
        "\n",
        "\n",
        "  #  Test the Training Step\n",
        "  translator = TrainTranslator(\n",
        "      embedding_dim, units,\n",
        "      input_text_processor=input_text_processor,\n",
        "      output_text_processor=output_text_processor,\n",
        "      use_tf_function=False)\n",
        "\n",
        "  # Configure the loss and optimizer\n",
        "  with tpu_strategy.scope(): \n",
        "    translator.compile(\n",
        "        optimizer=tf.optimizers.Adam(),\n",
        "        loss=MaskedLoss()\n",
        "    )\n",
        "  np.log(output_text_processor.vocabulary_size())\n",
        "\n",
        "  TrainTranslator._tf_train_step = _tf_train_step\n",
        "  translator.use_tf_function = True\n",
        "\n",
        "\n",
        "  #with tpu_strategy.scope(): \n",
        "  train_translator = TrainTranslator(\n",
        "      embedding_dim, units,\n",
        "      input_text_processor=input_text_processor,\n",
        "      output_text_processor=output_text_processor)\n",
        "\n",
        "  # Configure the loss and optimizer\n",
        "  train_translator.compile(\n",
        "      optimizer=tf.optimizers.Adam(),\n",
        "      loss=MaskedLoss(),\n",
        "  )\n",
        "\n",
        "  batch_loss = BatchLogs('batch_loss')\n",
        "\n",
        "  train_translator.fit(dataset, epochs=10, workers = 10, callbacks=[batch_loss])\n",
        "  \n",
        "  translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "  )\n",
        "  clusterModels.append(translator)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W8la0xUCfPVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7t5wpCfvMak"
      },
      "source": [
        "# Evaluate clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPYN4N0rg46L",
        "outputId": "a56fe1aa-19ed-40c8-88d2-4f7b711f4e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.26518382059800666, 0.2566192358803987, 0.27835119601328906, 0.2633577740863787, 0.25877705980066446, 0.2857981229235881, 0.25913732558139535, 0.35824759136212625, 0.2646114119601329, 0.2557845514950166, 0.26713367109634556, 0.3097483222591363, 0.2640554318936877, 0.2764529069767442, 0.33156091362126244]]\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "Translator.tokens_to_text = tokens_to_text\n",
        "Translator.sample = sample\n",
        "Translator.translate = translate_unrolled\n",
        "clusnum = 0\n",
        "for i in clusterModels:\n",
        "  temp = [MRR(i,testingResponses,testingQuestions,testingAnswers,False)]\n",
        "\n",
        "  testResp = \"drive/MyDrive/Cluster 15 \"+str(clusnum)+ \"Testing Responses\"\n",
        "\n",
        "  testAns = \"drive/MyDrive/Cluster 15\"+ str(clusnum)+ \"Testing Answers\"\n",
        "\n",
        "\n",
        "  generatedResponses, realAnswers =  ResponsesExport(i,testingResponses, testingQuestions, testingAnswers)\n",
        "  with open(testResp, 'x', encoding=\"utf-8\") as TestResponses:\n",
        "      for z in generatedResponses:\n",
        "          TestResponses.write(z+\"\\n\")\n",
        "\n",
        "  with open(testAns, 'x', encoding=\"utf-8\") as TestAnswers:\n",
        "      for z in realAnswers:\n",
        "          TestAnswers.write(z+\"\\n\")\n",
        "  scores.append(temp)\n",
        "  clusnum +=1\n",
        "scores = np.array(scores).transpose().tolist()\n",
        "print(scores)\n",
        "# winner = scores[1].index(max(scores[1]))\n",
        "# print(\"Winning RNN: \",winner +1, \", With Score: \", scores[1][winner])\n",
        "\n",
        "# StatFrame = pd.DataFrame(scores, columns=['1', '2','3','4','5'],index = [\"Testing\"])\n",
        "# print(StatFrame)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZuOidQ4LU46",
        "outputId": "e742fbf3-40b8-4c5f-d8b9-d6807077cb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.24509784053156147, 0.2511787209302325, 0.24930880398671096, 0.24926342192691028, 0.24419199335548172, 0.2591792026578073, 0.2585718438538206, 0.253692342192691, 0.24897006644518271, 0.251001146179402]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxUDKjfpLraI",
        "outputId": "88e6d4db-f64b-4ae6-a7d1-b4e2221b838f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Posed Question: \n",
            "Hi how are you?\n",
            "Decoded output: \n",
            "yes much i always if a while i think like it were bragging people know i dont know . i like if i astrid .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "translator = clusterModels[winner]\n",
        "Translator.tokens_to_text = tokens_to_text\n",
        "Translator.sample = sample\n",
        "Translator.translate = translate_unrolled\n",
        "\n",
        "\n",
        "input_text = tf.constant([\n",
        "    'Hi how are you?',\n",
        "])\n",
        "\n",
        "\n",
        "print(\"Posed Question: \")\n",
        "print(input_text.numpy()[0].decode())\n",
        "result = translator.translate(input_text)\n",
        "print(\"Decoded output: \")\n",
        "print(result[\"text\"].numpy()[0].decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ6gebXl6qC5"
      },
      "source": [
        "# Responses Exports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2i5xPmT6pWi"
      },
      "outputs": [],
      "source": [
        "def ResponsesExport(model,responses,questions,answers):\n",
        "  generatedResponses = []\n",
        "  realAnswers = []\n",
        "  model = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "  )\n",
        "\n",
        "  Translator.tokens_to_text = tokens_to_text\n",
        "  Translator.sample = sample\n",
        "  Translator.translate = translate_unrolled\n",
        "  for i in range(0,len(questions)):\n",
        "    \n",
        "    result = model.translate(questions[i])\n",
        "    generatedResponses.append(result[\"text\"].numpy()[0].decode()) \n",
        "    realAnswers.append(responses[i][answers[i]])\n",
        "\n",
        "  return generatedResponses, realAnswers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp6_c3vV_APa",
        "outputId": "d1974900-e8ac-4532-dc40-6d2e7f2a8252"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "generatedResponses, realAnswers =  ResponsesExport(translator,trainingResponses, trainingQuestions, trainingAnswers)\n",
        "with open('drive/MyDrive/Training Responses.txt', 'x') as TrainResponses:\n",
        "  for i in generatedResponses:\n",
        "    TrainResponses.write(i+\"\\n\")\n",
        "\n",
        "with open('drive/MyDrive/Training Answers.txt', 'x') as TrainAnswers:\n",
        "  for i in realAnswers:\n",
        "    TrainAnswers.write(i+\"\\n\")\n",
        "\n",
        "generatedResponses, realAnswers =  ResponsesExport(translator,testingResponses, testingQuestions, testingAnswers)\n",
        "with open('drive/MyDrive/Testing Responses.txt', 'x') as TestResponses:\n",
        "  for i in generatedResponses:\n",
        "    TestResponses.write(i+\"\\n\")\n",
        "\n",
        "with open('drive/MyDrive/Testing Answers.txt', 'x') as TestAnswers:\n",
        "  for i in realAnswers:\n",
        "    TestAnswers.write(i+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# demo"
      ],
      "metadata": {
        "id": "CF_ts7YJrfhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._train_step = _train_step\n",
        "TrainTranslator._loop_step = _loop_step\n",
        "# Encoder and Decoder Model\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "Decoder.call = call\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# # Configure the loss and optimizer\n",
        "# with tpu_strategy.scope(): \n",
        "#   translator.compile(\n",
        "#       optimizer=tf.optimizers.Adam(),\n",
        "#       loss=MaskedLoss()\n",
        "#   )\n",
        "# np.log(output_text_processor.vocabulary_size())\n",
        "\n",
        "# TrainTranslator._tf_train_step = _tf_train_step\n",
        "# translator.use_tf_function = True\n",
        "singleRNN = \"/content/drive/MyDrive/Single RNN/Model Checkpoints/cp-100.ckpt\"\n",
        "translator.load(singleRNN)\n",
        "\n",
        "# with tf.device('/device:GPU:0'):\n",
        "\n",
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")\n",
        "\n",
        "\n",
        "#Input Parsing\n",
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")\n",
        "\n",
        "Translator.tokens_to_text = tokens_to_text\n",
        "Translator.sample = sample\n",
        "Translator.translate = translate_unrolled\n",
        "# Translator.translate = translate_symbolic\n",
        "\n",
        "\n",
        "\n",
        "input_text = tf.constant([\n",
        "    # It is yes,\n",
        "    'Its a very unusual film youll agree with that',\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "result = translator.translate(input_text)\n",
        "print(\"Rawr Output: \")\n",
        "print(result)\n",
        "print(\"Decoded output: \")\n",
        "print(result[\"text\"].numpy()[0].decode())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "Ck_FwANBrfCU",
        "outputId": "8eae884b-d728-4bdd-e5ab-f1f54f56c439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3a12b0a106f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# translator.use_tf_function = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0msingleRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Single RNN/Model Checkpoints/cp-100.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingleRNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# with tf.device('/device:GPU:0'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'translator' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "wMzhEP7B14og",
        "qPcCyybFsVk2",
        "H7bj0PCvF6S9",
        "mJRP6C1dGDdo",
        "h9Fma4keu3PU",
        "Me9mp6D2GNxP",
        "Tu4Anv0oGWvk",
        "NcfutLUNGcLo"
      ],
      "name": "diss program.ipynb",
      "provenance": [],
      "mount_file_id": "1LcDSiWBPC3v-S9FslVydZCFGW0whiHVB",
      "authorship_tag": "ABX9TyPwpK1Dn15oFkbRVOH5allU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}